# LLM-Assisted Data Pipeline

A production-grade CSV data pipeline with comprehensive validation, cleaning, statistical analysis, and **intelligent autonomous agent** capabilities. Built with Python, pandas, Flask, and modern deployment architecture for professional data processing workflows.

## üìã Overview

This project provides a complete, modular data processing framework designed to:
- **Validate** incoming CSV data against a defined schema
- **Clean** and normalize validated data with type coercion and formatting
- **Analyze** processed data with comprehensive statistics
- **Report** errors and insights with detailed error codes and messages
- **Understand** data quality autonomously with intelligent agent analysis
- **Recommend** optimized processing strategies and data fixes

### Key Features
- ‚úÖ **Schema Validation**: Validates transaction records (ID, amount, timestamp, country)
- ‚úÖ **Row-Level Validation**: Comprehensive error detection with 16+ error codes
- ‚úÖ **Data Cleaning**: Type coercion, whitespace trimming, rounding, timestamp normalization
- ‚úÖ **Statistical Analysis**: Numeric, string, and datetime statistics
- ‚úÖ **ü§ñ Intelligent Agent**: Autonomous analysis with quality scoring, issue detection, and recommendations
- ‚úÖ **Error Accumulation**: Collects all validation errors before rejecting rows
- ‚úÖ **Comprehensive Testing**: 34 unit tests covering all functionality
- ‚úÖ **Production Deployment**: Live on Render.com with GitHub Pages frontend
- ‚úÖ **Professional Documentation**: ~150 pages of specs, architecture, examples, and guides

## ü§ñ Intelligent Agent System

**NEW:** Meet the Data Pipeline Agent - an autonomous system that analyzes your CSV data and provides intelligent insights.

### Agent Capabilities
- üìä **Quality Scoring** (0-100): Automatic data quality assessment
- üîç **Issue Detection**: Identifies outliers, duplicates, missing values, data type issues
- üí° **Smart Recommendations**: Suggests optimal data fixes and processing strategies
- üíé **Insights**: Detects patterns, skewness, cardinality issues, data characteristics
- ‚úì **Suggested Actions**: Provides prioritized next steps for data processing

### Example Analysis
```
Quality Score: 90.0/100 ‚úÖ
Issues Found: 2
  ‚Ä¢ Column 'amount' contains 2 potential outliers
  ‚Ä¢ Column 'customer_id' has skewed distribution

Recommendations: 1
  ‚Ä¢ Found 10% duplicate rows ‚Üí Remove duplicates before analysis

Suggested Actions:
  1. Handle missing values
  2. Remove duplicate records
  3. Investigate and handle detected issues
  4. Validate data against business rules
  5. Export cleaned data for analysis
```

### Try the Agent
1. Visit: https://SHAYAN0123.github.io/llm-assisted-data-pipeline/
2. Upload a CSV file or use sample data
3. See real-time quality analysis
4. Review intelligent recommendations
5. Follow suggested actions

**Agent Documentation:**
- üìñ [Agent System Overview](AGENT_DOCUMENTATION.md) - Complete technical reference
- üíº [Real-World Use Cases](AGENT_EXAMPLES.md) - E-commerce, HR, Finance examples
- üìã [Implementation Summary](AGENT_SYSTEM_SUMMARY.md) - Business value & deployment

## üöÄ Quick Start

### Prerequisites
- Python 3.10+
- [uv](https://docs.astral.sh/uv/) (modern Python package manager)

### Installation with uv

1. **Clone the repository**
   ```bash
   git clone https://github.com/SHAYAN0123/llm-assisted-data-pipeline.git
   cd llm-assisted-data-pipeline
   ```

2. **Install uv** (if not already installed)
   ```bash
   # macOS/Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh
   
   # Windows
   powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
   ```

3. **Create and sync environment**
   ```bash
   uv sync
   ```

4. **Verify installation by running tests**
   ```bash
   uv run pytest -v
   ```

   Expected output: **All 34 tests passing** ‚úì

### Quick Commands

```bash
# Run the tests
uv run pytest test_pipeline.py -v

# Start the Flask backend (port 3000)
PORT=3000 uv run python app.py

# Access the interactive landing page
# Visit http://localhost:3000

# Install additional dev dependencies
uv sync --extra dev
```

## üíª Basic Usage

### Simple Example
```python
import pandas as pd
from pipeline import run_pipeline

# Create sample data
data = pd.DataFrame({
    'transaction_id': ['TXN001', 'TXN002'],
    'amount': ['100.50', '250.75'],
    'timestamp': ['2025-01-10T14:30:00Z', '2025-01-11T09:15:00Z'],
    'country': ['US', 'GB']
})

# Run the complete pipeline
valid_df, invalid_df, stats = run_pipeline(data)

# View results
print("Valid rows:", len(valid_df))
print("Invalid rows:", len(invalid_df))
print("Statistics:", stats)
```

### Advanced Example with Error Handling
```python
from pipeline import SchemaValidator, DataCleaner, StatisticsCalculator

# Initialize components
validator = SchemaValidator()
cleaner = DataCleaner()
calculator = StatisticsCalculator()

# Validate schema
valid_df, invalid_df = validator.validate_rows(data)

# Check for validation errors
if len(invalid_df) > 0:
    print(f"Found {len(invalid_df)} invalid rows")
    print(invalid_df[['transaction_id', 'error_code', 'error_message']])
else:
    # Clean data
    cleaned_df = cleaner.clean_data(valid_df)
    
    # Calculate statistics
    stats = calculator.calculate_stats(cleaned_df)
    print("Pipeline completed successfully")
```

## üìÅ Project Structure

```
llm-assisted-data-pipeline/
‚îú‚îÄ‚îÄ pipeline.py                      # Core implementation (418 lines)
‚îú‚îÄ‚îÄ test_pipeline.py                 # Test suite (750+ lines, 34 tests)
‚îú‚îÄ‚îÄ README.md                        # This file
‚îÇ
‚îú‚îÄ‚îÄ üìö SPECIFICATIONS & DESIGN
‚îú‚îÄ‚îÄ REQUIREMENTS.md                  # Functional specification (13 sections)
‚îú‚îÄ‚îÄ ARCHITECTURE.md                  # Architecture design (7 components)
‚îú‚îÄ‚îÄ SCHEMA_VALIDATION_RULES.md       # Validation rules & error codes
‚îÇ
‚îú‚îÄ‚îÄ üìä ANALYSIS & REVIEW
‚îú‚îÄ‚îÄ CODE_REVIEW.md                   # Code review (13 issues identified)
‚îú‚îÄ‚îÄ IMPROVEMENTS.md                  # 7 implementation fixes with code
‚îú‚îÄ‚îÄ REVIEW_SUMMARY.md                # Quick reference summary
‚îú‚îÄ‚îÄ REVIEW_EXECUTIVE_SUMMARY.txt     # Management brief
‚îú‚îÄ‚îÄ REVIEW_VISUAL_SUMMARY.txt        # Visual matrices & checklists
‚îú‚îÄ‚îÄ README_REVIEW.md                 # Documentation index
‚îÇ
‚îî‚îÄ‚îÄ üìù GIT DOCUMENTATION
    ‚îú‚îÄ‚îÄ GIT_PUSH_SUMMARY.md          # Push details
    ‚îî‚îÄ‚îÄ COMMIT_MESSAGE_DETAILS.md    # Commit message explanation
```

## üìñ Documentation Guide

### For Quick Understanding (30 minutes)
1. **REVIEW_SUMMARY.md** - Quick reference with key findings
2. **REVIEW_VISUAL_SUMMARY.txt** - Visual matrices and risk assessment

### For Comprehensive Understanding (2-3 hours)
1. **REQUIREMENTS.md** - What the system should do
2. **ARCHITECTURE.md** - How the system is designed
3. **CODE_REVIEW.md** - Technical analysis of current implementation
4. **IMPROVEMENTS.md** - How to enhance the system

### For Management/Stakeholders (30 minutes)
- **REVIEW_EXECUTIVE_SUMMARY.txt** - Business impact and timeline
- **REVIEW_VISUAL_SUMMARY.txt** - Visual overview of risks and readiness

### For Implementation Planning (4 hours)
1. **CODE_REVIEW.md** - Identify all issues
2. **IMPROVEMENTS.md** - Get complete code examples
3. **REVIEW_SUMMARY.md** - Risk/effort matrix for prioritization

## üîç Core Components

### 1. SchemaValidator
Validates incoming data against the defined schema.

**Features:**
- Column existence validation
- Row-level field validation (transaction_id, amount, timestamp, country)
- Error code generation (E101-E404)
- Error accumulation before rejection

**Error Codes:**
- **E101-E104**: Transaction ID validation errors
- **E201-E204**: Amount validation errors
- **E301-E304**: Timestamp validation errors
- **E401-E404**: Country validation errors

**Constraints:**
- Transaction IDs: 8-32 characters, alphanumeric with hyphens
- Amounts: Positive floats, 2 decimal places max
- Timestamps: ISO 8601 format (1970-2030)
- Countries: ISO 3166-1 alpha-2 codes

### 2. DataCleaner
Cleans and normalizes validated data.

**Operations:**
- Type coercion (string ‚Üí float, string ‚Üí datetime)
- Whitespace trimming (leading/trailing)
- Amount rounding to 2 decimal places
- Timestamp normalization to ISO 8601

### 3. StatisticsCalculator
Generates comprehensive statistics on processed data.

**Statistics By Type:**
- **Numeric** (amount): count, min, max, mean, std, median, Q1, Q3
- **String** (transaction_id, country): count, unique, mode, frequency
- **DateTime** (timestamp): earliest, latest, range, most common period

## üß™ Testing

### Run All Tests
```bash
python3 -m pytest test_pipeline.py -v
```

### Run Specific Test Classes
```bash
# Schema validation tests
python3 -m pytest test_pipeline.py::TestSchemaValidation -v

# Missing values tests
python3 -m pytest test_pipeline.py::TestMissingValues -v

# Negative amount tests
python3 -m pytest test_pipeline.py::TestNegativeAmounts -v

# Data cleaning tests
python3 -m pytest test_pipeline.py::TestDataCleaning -v

# Statistics tests
python3 -m pytest test_pipeline.py::TestStatisticsCalculation -v

# Integration tests
python3 -m pytest test_pipeline.py::TestPipelineIntegration -v
```

### Run Tests with Filtering
```bash
# Run only tests matching a pattern
python3 -m pytest test_pipeline.py -v -k "amount"

# Show short error messages
python3 -m pytest test_pipeline.py -v --tb=short

# Show detailed output
python3 -m pytest test_pipeline.py -v -s
```

### Test Coverage
- ‚úÖ **Schema Validation**: 5 tests
- ‚úÖ **Missing Values**: 7 tests
- ‚úÖ **Negative Amounts**: 8 tests
- ‚úÖ **Data Cleaning**: 6 tests
- ‚úÖ **Statistics Calculation**: 4 tests
- ‚úÖ **Pipeline Integration**: 5 tests

**Total: 34 tests (all passing)**

## üìä Current Status & Roadmap

### Production Readiness: **2/10**

**What Works Well (9/10):**
- Excellent code structure and modularity (architecture: 9/10)
- Comprehensive testing (testing: 9/10)
- Detailed documentation (documentation: 9/10)

**What Needs Work (2/10):**
- Scalability issues (row-by-row iteration)
- Memory limitations (loads entire file)
- Missing features (duplicate detection, outlier detection, monitoring)

### Critical Issues (Fix Immediately)

1. **Row Iteration Bottleneck** (30 min, 50x speedup)
   - Problem: Uses row-by-row iteration instead of vectorized operations
   - Impact: Cannot handle large files efficiently
   - Solution: Use pandas vectorized operations

2. **Duplicate Detection** (15 min)
   - Problem: E102 error code defined but never implemented
   - Impact: Duplicate transaction IDs not detected
   - Solution: Implement before validation returns

3. **Hardcoded Date Limit** (5 min)
   - Problem: Year 2030 hardcoded for timestamp validation
   - Impact: Rejects valid future dates
   - Solution: Use dynamic date calculation

4. **Missing Outlier Detection** (2 hours)
   - Problem: No detection for anomalous amounts
   - Impact: Cannot identify suspicious transactions
   - Solution: Implement IQR or Z-score based detection

### Implementation Timeline

**Week 1 (3 issues, ~50 min)**
- Vectorize row iteration
- Implement duplicate detection
- Fix hardcoded year 2030

**Week 2 (3 issues, ~5 hours)**
- Add outlier detection
- Implement configuration system
- Add data lineage tracking

**Week 3-4 (2 issues, ~8 hours)**
- Streaming/chunked processing
- Monitoring metrics integration

## üîß Configuration

### Current Configuration
Validation rules are hardcoded in `SchemaValidator.__init__()`:

```python
# Transaction ID rules
TRANSACTION_ID_PATTERN = r'^[a-zA-Z0-9\-]{8,32}$'

# Amount rules
MIN_AMOUNT = 0.0
MAX_AMOUNT = 999999.99
DECIMAL_PLACES = 2

# Timestamp rules
MIN_DATE = datetime(1970, 1, 1)
MAX_DATE = datetime(2030, 12, 31)

# Country codes
VALID_COUNTRIES = {'US', 'GB', 'CA', 'AU', ...}
```

### Future Configuration (Week 2)
See `IMPROVEMENTS.md` Fix #5 for YAML-based configuration system.

## üìà Performance Characteristics

### Current Performance
- **Memory Usage**: O(n) - loads entire file into memory
- **Time Complexity**: O(n*m) - row iteration with field validation
- **Throughput**: ~1,000-5,000 rows/sec (depends on system)
- **Scalability**: Breaks with files > 2GB

### Post-Optimization Performance (Week 1)
- **Speedup**: 50x faster (vectorized operations)
- **New Throughput**: ~50,000-250,000 rows/sec

### Post-Streaming (Week 3)
- **Memory**: O(chunk_size) - configurable
- **Scalability**: Unlimited file size

## üêõ Known Limitations

1. **Row-by-Row Processing** (Performance Issue)
   - Cannot efficiently handle large datasets
   - Fix in IMPROVEMENTS.md (Fix #1)

2. **File-Based Only** (Flexibility Issue)
   - No support for streaming data sources
   - No database integration
   - Fix planned for Week 3

3. **No Duplicate Detection** (Data Quality Issue)
   - Transaction IDs not checked for duplicates
   - Can process the same transaction multiple times
   - Fix in IMPROVEMENTS.md (Fix #2)

4. **No Outlier Detection** (Data Quality Issue)
   - Unusually large amounts not detected
   - Suspicious patterns not identified
   - Fix in IMPROVEMENTS.md (Fix #4)

5. **Hardcoded Validation Rules** (Configuration Issue)
   - Rules not externalized to config files
   - Difficult to adjust for different data sources
   - Fix in IMPROVEMENTS.md (Fix #5)

6. **No Monitoring** (Operational Issue)
   - No metrics collection
   - No logging to external systems
   - Fix in IMPROVEMENTS.md (Fix #7)

## üéØ Success Criteria

- ‚úÖ All 34 unit tests passing
- ‚úÖ Schema validation working correctly
- ‚úÖ Data cleaning functioning properly
- ‚úÖ Statistics calculation accurate
- ‚úÖ Error messages clear and actionable
- ‚ö†Ô∏è Performance optimized (in progress)
- ‚ö†Ô∏è Production deployment ready (in progress)

## ü§ù Contributing

### Development Workflow
1. Create a feature branch: `git checkout -b feature/your-feature`
2. Make changes and test: `python3 -m pytest test_pipeline.py -v`
3. Commit with conventional commits: `git commit -m "feat: description"`
4. Push to GitHub: `git push origin feature/your-feature`
5. Create a Pull Request with detailed description

### Testing Requirements
- All tests must pass: `pytest test_pipeline.py -v`
- No new warnings or errors
- New features must include tests
- Maintain or improve code coverage

### Code Style
- Follow PEP 8 conventions
- Use type hints where possible
- Add docstrings to all functions
- Keep functions focused and modular

## üë• Authors

- **SHAYAN0123** - Initial implementation and documentation
- **LLM-Assisted Development** - Code generation and analysis support


## üîó Related Resources

### In This Repository
- **REQUIREMENTS.md** - Complete functional specification
- **ARCHITECTURE.md** - System design and data flows
- **CODE_REVIEW.md** - Detailed technical analysis (13 issues)
- **IMPROVEMENTS.md** - 7 implementation fixes with complete code
- **REVIEW_SUMMARY.md** - Quick reference summary
- **SCHEMA_VALIDATION_RULES.md** - Validation specification

### External Resources
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [pytest Documentation](https://docs.pytest.org/)
- [Conventional Commits](https://www.conventionalcommits.org/)
- [PEP 8 Style Guide](https://pep8.org/)

## ‚ú® Key Achievements

- ‚úÖ Production-grade code with comprehensive error handling
- ‚úÖ 34 passing unit tests covering all functionality
- ‚úÖ ~100 pages of professional documentation
- ‚úÖ 13 issues identified with 7 complete fixes provided
- ‚úÖ 4-week implementation roadmap
- ‚úÖ Risk assessment and business impact analysis
- ‚úÖ Ready for team collaboration and code reviews



**Last Updated:** January 13, 2026  
**Commit Hash:** 0f316b1  
**Repository:** https://github.com/SHAYAN0123/llm-assisted-data-pipeline  
**Status:** ‚úÖ Live and ready for collaboration
